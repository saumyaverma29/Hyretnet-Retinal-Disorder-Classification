{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13586296,"sourceType":"datasetVersion","datasetId":8631833}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport joblib \nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, Concatenate,\n    GlobalAveragePooling2D, GlobalAveragePooling1D, BatchNormalization, Activation\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications import EfficientNetB4\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport transformers\nimport logging\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nfrom transformers import TFSwinModel\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. DATA LOADING AND PREPROCESSING ---\ndataset_path = \"/kaggle/input/minor-dataset/Data_Minor\"\nclasses = [\"HR\", \"DR\", \"RVO\"]\nimg_size = (224, 224)\n\ndef apply_clahe(image):\n    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    l = clahe.apply(l)\n    lab = cv2.merge((l, a, b))\n    return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n\ndef adjust_gamma(image, gamma=1.0):\n    invGamma = 1.0 / gamma\n    table = np.array([(i / 255.0) ** invGamma * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n    return cv2.LUT(image, table)\n\ndef adaptive_gamma_correction(image):\n    mean_intensity = np.mean(image) / 255.0\n    gamma = 1.2 if mean_intensity < 0.5 else 0.9\n    return adjust_gamma(image, gamma)\n\ndef preprocess_image(image):\n    image = apply_clahe(image)\n    image = adaptive_gamma_correction(image)\n    return image.astype(np.float32) / 255.0\n\ndef load_images():\n    images, labels = [], []\n    for label, cls in enumerate(classes):\n        class_dir = os.path.join(dataset_path, cls)\n        for img_name in sorted(os.listdir(class_dir)):\n            img_path = os.path.join(class_dir, img_name)\n            try:\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, img_size)\n                image = preprocess_image(image)\n                images.append(image)\n                labels.append(label)\n            except Exception as e:\n                pass\n    return np.array(images), np.array(labels)\n\nimages, labels = load_images()\nprint(f\"Dataset Loaded: {images.shape}, Labels: {labels.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. 70/10/20 TRAIN/VAL/TEST SPLIT ---\nnum_classes = len(classes)\ninput_shape = (224, 224, 3)\nbatch_size = 8\nepochs = 30\n\ntrain_val_images, test_images, train_val_labels, test_labels = train_test_split(\n    images, labels, test_size=0.2, stratify=labels, random_state=42\n)\ntrain_images, val_images, train_labels, val_labels = train_test_split(\n    train_val_images, train_val_labels, test_size=0.125, stratify=train_val_labels, random_state=42\n)\nprint(f\"Training: {len(train_images)}, Validation: {len(val_images)}, Test: {len(test_images)}\")\n\ntrain_labels_cat = to_categorical(train_labels, num_classes=num_classes)\nval_labels_cat = to_categorical(val_labels, num_classes=num_classes)\ntest_labels_cat = to_categorical(test_labels, num_classes=num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. MODEL DEFINITIONS ---\ntf.config.run_functions_eagerly(True)\n\ndef build_cnn_feature_extractor(input_shape):\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape), MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(64, (3, 3), activation='relu'), MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(128, (3, 3), activation='relu'), MaxPooling2D(pool_size=(2, 2)),\n        Flatten(), Dense(128, activation='relu'), Dropout(0.5)\n    ], name=\"cnn_feature_extractor\")\n    return model\n\nclass SwinWrapper(tf.keras.layers.Layer):\n    def __init__(self, **kwargs): super().__init__(**kwargs); self.swin_model = None\n    def build(self, input_shape): self.swin_model = TFSwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\"); super().build(input_shape)\n    def call(self, inputs): x = tf.transpose(inputs, perm=[0, 3, 1, 2]); outputs = self.swin_model(x); return outputs.last_hidden_state\n\ndef build_swin_feature_extractor(input_shape):\n    input_layer = Input(shape=input_shape); swin_out = SwinWrapper()(input_layer); pooled_output = GlobalAveragePooling1D()(swin_out); return Model(inputs=input_layer, outputs=pooled_output, name=\"swin_feature_extractor\")\n\ndef build_hybrid_model(input_shape, num_classes):\n    input_layer = Input(shape=input_shape, name=\"input_image\")\n    cnn_extractor = build_cnn_feature_extractor(input_shape)\n    cnn_features = cnn_extractor(input_layer)\n    swin_extractor = build_swin_feature_extractor(input_shape)\n    swin_features = swin_extractor(input_layer)\n    combined_features = Concatenate()([cnn_features, swin_features])\n    x = Dense(128, activation='relu')(combined_features)\n    x = Dropout(0.5)(x)\n    output = Dense(num_classes, activation='softmax', name='output_layer')(x)\n    model = Model(inputs=input_layer, outputs=output, name=\"HyRetNet\")\n    return model\n\ndef build_resnet(input_shape, num_classes):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = False \n    inputs = Input(shape=input_shape)\n    x = base_model(inputs, training=False)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs, outputs, name=\"ResNet50\")\n    return model\n\n\ndef build_efficientnet(input_shape, num_classes):\n    base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = True\n    for layer in base_model.layers[:-40]:\n        layer.trainable = False\n        \n    inputs = Input(shape=input_shape)\n    x = base_model(inputs, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = BatchNormalization()(x) # Added stability\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n    \n    model = Model(inputs, outputs, name=\"EfficientNetB4\")\n    return model\n\n\n# --- 4. EXPERIMENT EXECUTION ---\n\nmodels_to_run = {\n    \"HyRetNet\": build_hybrid_model,\n    \"ResNet50\": build_resnet,               \n    \"EfficientNetB4\": build_efficientnet \n}\n\nresults_dir = \"/kaggle/working/model_results/\"\nos.makedirs(results_dir, exist_ok=True)\n\nfor model_name, model_builder in models_to_run.items():\n    print(f\"\\n\" + \"=\"*50)\n    print(f\"--- Checking Model: {model_name} ---\")\n    print(\"=\"*50)\n    \n    result_file = os.path.join(results_dir, f\"results_{model_name}.joblib\")\n    model_file = os.path.join(results_dir, f\"best_model_{model_name}.h5\")\n\n    if os.path.exists(result_file):\n        print(f\"Results file found for {model_name}. Skipping training.\")\n        continue \n    \n    print(f\"No results found. Training {model_name}...\")\n    \n    model = model_builder(input_shape, num_classes)\n    \n    lr = 0.00005 if \"EfficientNet\" in model_name else 0.0001\n    \n    model.compile(\n        optimizer=Adam(learning_rate=lr),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    checkpoint = ModelCheckpoint(model_file, monitor='val_loss', save_best_only=True, mode='min', verbose=0)\n    \n    # Train\n    history = model.fit(\n        train_images, train_labels_cat,\n        epochs=epochs,\n        validation_data=(val_images, val_labels_cat),\n        callbacks=[checkpoint],\n        batch_size=batch_size,\n        verbose=1\n    )\n    \n    # Load best model for TEST evaluation\n    custom_objs = {\"SwinWrapper\": SwinWrapper} if \"Swin\" in model_name or \"HyRetNet\" in model_name else {}\n    best_model = tf.keras.models.load_model(model_file, custom_objects=custom_objs)\n        \n    print(f\"\\nEvaluating {model_name} on the TEST SET...\")\n    test_loss, test_accuracy = best_model.evaluate(test_images, test_labels_cat, verbose=1)\n    \n    print(f\"\\nFINAL TEST ACCURACY for {model_name}: {test_accuracy*100:.2f}%\")\n    \n    predictions = best_model.predict(test_images)\n    predicted_labels = np.argmax(predictions, axis=1)\n    \n    model_result_data = {\n        \"history\": history.history,\n        \"test_loss\": test_loss,\n        \"test_accuracy\": test_accuracy,\n        \"predicted_labels\": predicted_labels,\n        \"report\": classification_report(test_labels, predicted_labels, target_names=classes, output_dict=True),\n        \"params\": model.count_params()\n    }\n    \n    joblib.dump(model_result_data, result_file)\n    print(f\"--- {model_name} Complete. ---\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. PLOTTING (CLEAN STYLE) ---\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"--- EXPERIMENT COMPLETE: LOADING ALL RESULTS ---\")\nprint(\"=\"*60)\n\nresults = {}\nfor model_name in models_to_run.keys():\n    result_file = os.path.join(results_dir, f\"results_{model_name}.joblib\")\n    if os.path.exists(result_file):\n        results[model_name] = joblib.load(result_file)\n\n# Plot everything\nfor model_name, data in results.items():\n    history = data['history']\n    \n    plt.figure(figsize=(14, 5))\n    \n    # Subplot 1: Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history['loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.title('Loss Curve', fontsize=14)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.ylim(bottom=0) \n\n    # Subplot 2: Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history['accuracy'], label='Train Accuracy')\n    plt.plot(history['val_accuracy'], label='Val Accuracy')\n    plt.title('Accuracy Curve', fontsize=14)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.ylim([0.65, 1.0]) \n    \n    plt.suptitle(f'Training History for {model_name}', fontsize=16, y=1.03)\n    plt.tight_layout()\n    plt.show()\n\n# Final Bar Chart\nsummary_data = {\n    \"Model\": [name for name in results.keys()],\n    \"Test Accuracy\": [data['test_accuracy'] for data in results.values()]\n}\nsummary_df = pd.DataFrame(summary_data).sort_values(by=\"Test Accuracy\", ascending=False)\n\nplt.figure(figsize=(12, 7))\nbars = plt.bar(summary_df[\"Model\"], summary_df[\"Test Accuracy\"], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\nplt.ylabel('Test Accuracy Score')\nplt.title('FINAL Model Test Accuracy Comparison', fontsize=16)\nplt.xticks(rotation=15)\nplt.bar_label(bars, fmt='%.4f')\nplt.ylim(top=plt.ylim()[1] * 1.05)\nplt.show()\n\nprint(\"\\n\\n--- FINAL SUMMARY TABLE ---\")\nprint(summary_df.to_markdown(index=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}